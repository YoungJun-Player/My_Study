{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. 임베딩이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화만으로는 모델을 학습할 수 없다 \n",
    "\n",
    "컴퓨터는 텍스트 자체를 이해할 수 없으므로 텍스트를 숫자로 변환하는 텍스트 벡터화 과정이 필요하다\n",
    "\n",
    "텍스트 벡터화란 텍스트를 숫자로 변환하는 과정을 의미한다\n",
    "\n",
    "기초적인 텍스트 벡터화로는 원-핫 인코딩, 빈도 벡터화 등이 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원-핫 인코딩은 단어가 여러번 등장하더라도 0과 1로 나누지만 빈도벡터화는 해당 단어의 빈도로 표시한다\n",
    "\n",
    "이러한 방법은 단어나 문장을 벡터 형태로 변환하기 쉽고 간단하다는 장점이 있지만 벡터의 희소성이 크다는 단점이 있다\n",
    "\n",
    "말뭉치 내에 존재하는 토큰의 개수 만큼의 벡터 차원을 가져야 하지만 입력 문장 내에 존재하는 토큰의 수는 그에 비해 현저히 적기 때문에 컴퓨팅 비용 증가와 차원의 저주와 같은 문제를 겪을 수 있다\n",
    "\n",
    "또한 텍스트의 벡터가 입력 텍스트의 의미를 내포하고 있지 않으므로 두 문장이 의미적으로 유사하다고 해도 벡터가 유사하게 나타나지 않을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 문제를 해결하기 위해 워드 투 벡터(Word2Vec)나 패스트 텍스트(fastText) 등과 같이 단어의 의미를 학습해 표현하는 워드 임베딩(Word Embedding) 기법을 사용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 임베딩 기법은 단어를 고정된 길이의 실수 벡터를 표현하는 방법으로, 단어의 의미를 벡터 공간에서 다른 단어와의 상대적 위치로 표현해 단어 간의 관계를 추론한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 임베딩은 고정된 임베딩을 학습하기 때문에 다의어나 문맥 정보를 다루기 어렵다는 단점이 있어 인공 신경망을 활용해 동적 임베딩(Dynamic Embedding) 기법을 사용한다"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
